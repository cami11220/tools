# Es difícil estandarizar web scraping porque depende de cada página
!pip install playwright
!playwright install

import pandas as pd
import asyncio
from playwright.async_api import async_playwright

# Lista de valores extraídos de las etiquetas <option>
valores = ["1096", "1097"]

# Crear la lista de URLs
urls = ["https://www.camara.cl/diputados/detalle/gastosoperacionales.aspx?prmId=" + value for value in valores]

# Generar la lista de meses
meses = [str(i) for i in range(1, 13)]
df = []

async def scrape_url(page, url):
    global df
    try:
        await page.goto(url)

        await asyncio.wait_for(
            page.select_option('select[name="ctl00$ctl00$ctl00$ContentPlaceHolder1$ContentPlaceHolder1$DetallePlaceHolder$ddlAno"]', "2023"),
            timeout=10)

        for mes in meses:
            try:
                await asyncio.wait_for(
                    page.select_option('select[name="ctl00$ctl00$ctl00$ContentPlaceHolder1$ContentPlaceHolder1$DetallePlaceHolder$ddlMes"]', mes),
                    timeout=10
                )

                await page.wait_for_load_state("networkidle")
                await page.wait_for_selector('table.tabla tbody tr', state='visible')

                await asyncio.sleep(1)

                # Extraer datos de la tabla
                rows = await page.query_selector_all('table.tabla tbody tr')
                for row in rows:
                    columns = await row.query_selector_all('td')
                    if len(columns) >= 2:
                        gasto = (await columns[0].inner_text()).strip()
                        monto = (await columns[1].inner_text()).strip()
                        df.append({"URL": url, "Mes": mes, "Año": "2023", "Tipo de Gasto": gasto, "Monto": monto})

            except asyncio.TimeoutError:
                continue

    except Exception as e:
        print(f"Error al procesar URL {url}: {str(e)}")

#######################################################################################
async def run(playwright, urls):
    browser = await playwright.chromium.launch(headless=True)
    context = await browser.new_context()

    try:
        page = await context.new_page()

        for url in urls:
            await scrape_url(page, url)

    finally:
        await context.close()
        await browser.close()

async def main():
    async with async_playwright() as playwright:
        await run(playwright, urls)

# Ejecutar el bucle de eventos
await main()


df = pd.DataFrame(df)

# Guardar el DataFrame en un archivo CSV
df.to_csv('gastos_dip_data.csv', index=False)
